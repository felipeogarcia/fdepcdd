<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Aprendizado de Máquina | Fundamentos de Estatística para Ciência de Dados</title>
  <meta name="description" content="Este é o livro da disciplina Fundamentos de Estatística para Ciência de Dados da PUCRS-UOL escrito no RStudio. Sinta-se à vontade para compartilhar e colaborar https://github.com/filipezabala/fdepcdd" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Aprendizado de Máquina | Fundamentos de Estatística para Ciência de Dados" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Este é o livro da disciplina Fundamentos de Estatística para Ciência de Dados da PUCRS-UOL escrito no RStudio. Sinta-se à vontade para compartilhar e colaborar https://github.com/filipezabala/fdepcdd" />
  <meta name="github-repo" content="filipezabala/fdepcdd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Aprendizado de Máquina | Fundamentos de Estatística para Ciência de Dados" />
  
  <meta name="twitter:description" content="Este é o livro da disciplina Fundamentos de Estatística para Ciência de Dados da PUCRS-UOL escrito no RStudio. Sinta-se à vontade para compartilhar e colaborar https://github.com/filipezabala/fdepcdd" />
  

<meta name="author" content="Filipe J. Zabala" />


<meta name="date" content="2020-11-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modelos-lineares.html"/>
<link rel="next" href="seriestemporais.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<link href="libs/rglwidgetClass-2/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-2/rglClass.src.js"></script>
<script src="libs/CanvasMatrix4-2016/CanvasMatrix.src.js"></script>
<script src="libs/rglWebGL-binding-0.100.54/rglWebGL.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a></li>
<li class="chapter" data-level="1" data-path="introducao.html"><a href="introducao.html"><i class="fa fa-check"></i><b>1</b> Introdução</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introducao.html"><a href="introducao.html#ferramentas"><i class="fa fa-check"></i><b>1.1</b> Ferramentas</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introducao.html"><a href="introducao.html#r"><i class="fa fa-check"></i><b>1.1.1</b> R</a></li>
<li class="chapter" data-level="1.1.2" data-path="introducao.html"><a href="introducao.html#rstudio"><i class="fa fa-check"></i><b>1.1.2</b> RStudio</a></li>
<li class="chapter" data-level="1.1.3" data-path="introducao.html"><a href="introducao.html#python"><i class="fa fa-check"></i><b>1.1.3</b> Python</a></li>
<li class="chapter" data-level="1.1.4" data-path="introducao.html"><a href="introducao.html#jasp"><i class="fa fa-check"></i><b>1.1.4</b> JASP</a></li>
<li class="chapter" data-level="1.1.5" data-path="introducao.html"><a href="introducao.html#stan"><i class="fa fa-check"></i><b>1.1.5</b> Stan</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introducao.html"><a href="introducao.html#materiais-de-apoio"><i class="fa fa-check"></i><b>1.2</b> Materiais de apoio</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introducao.html"><a href="introducao.html#página-do-professor-filipe-zabala"><i class="fa fa-check"></i><b>1.2.1</b> Página do professor Filipe Zabala</a></li>
<li class="chapter" data-level="1.2.2" data-path="introducao.html"><a href="introducao.html#khan-academy"><i class="fa fa-check"></i><b>1.2.2</b> Khan Academy</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introducao.html"><a href="introducao.html#algarismos-e-numeros"><i class="fa fa-check"></i><b>1.3</b> Algarismos e Números</a></li>
<li class="chapter" data-level="1.4" data-path="introducao.html"><a href="introducao.html#porcentagens-decimais-e-milhares"><i class="fa fa-check"></i><b>1.4</b> Porcentagens, Decimais e Milhares</a></li>
<li class="chapter" data-level="1.5" data-path="introducao.html"><a href="introducao.html#o-senhor-x"><i class="fa fa-check"></i><b>1.5</b> O Senhor <span class="math inline">\(X\)</span></a></li>
<li class="chapter" data-level="1.6" data-path="introducao.html"><a href="introducao.html#somatorio"><i class="fa fa-check"></i><b>1.6</b> Somatório</a></li>
<li class="chapter" data-level="1.7" data-path="introducao.html"><a href="introducao.html#arredondamento-e-truncagem"><i class="fa fa-check"></i><b>1.7</b> Arredondamento e Truncagem</a></li>
<li class="chapter" data-level="1.8" data-path="introducao.html"><a href="introducao.html#outros"><i class="fa fa-check"></i><b>1.8</b> Outros símbolos e expressões</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="introducao.html"><a href="introducao.html#alfabeto-grego"><i class="fa fa-check"></i><b>1.8.1</b> Alfabeto grego</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html"><i class="fa fa-check"></i><b>2</b> Estatística Descritiva</a>
<ul>
<li class="chapter" data-level="2.1" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#variáveis"><i class="fa fa-check"></i><b>2.1</b> Variáveis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#variavel-qualitativa-nominal"><i class="fa fa-check"></i><b>2.1.1</b> Variável qualitativa nominal</a></li>
<li class="chapter" data-level="2.1.2" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#variavel-qualitativa-ordinal"><i class="fa fa-check"></i><b>2.1.2</b> Variável qualitativa ordinal</a></li>
<li class="chapter" data-level="2.1.3" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#variavel-quantitativa-discreta"><i class="fa fa-check"></i><b>2.1.3</b> Variável quantitativa discreta</a></li>
<li class="chapter" data-level="2.1.4" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#variavel-quantitativa-continua"><i class="fa fa-check"></i><b>2.1.4</b> Variável quantitativa contínua</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#distribuicao-de-frequencia"><i class="fa fa-check"></i><b>2.2</b> Distribuição de Frequência</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#db-rol-eo"><i class="fa fa-check"></i><b>2.2.1</b> Dados brutos, Rol e Estatísticas de Ordem</a></li>
<li class="chapter" data-level="2.2.2" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#tab-freq-univ-discr"><i class="fa fa-check"></i><b>2.2.2</b> Tabela de frequência univariada discreta</a></li>
<li class="chapter" data-level="2.2.3" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#tab-freq-univ-cont"><i class="fa fa-check"></i><b>2.2.3</b> Tabela de frequência univariada contínua</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#posicao"><i class="fa fa-check"></i><b>2.3</b> Medidas de Posição (ou Localização)</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#minmax"><i class="fa fa-check"></i><b>2.3.1</b> Mínimo e Máximo</a></li>
<li class="chapter" data-level="2.3.2" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#mas"><i class="fa fa-check"></i><b>2.3.2</b> Média (Aritmética Simples)</a></li>
<li class="chapter" data-level="2.3.3" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#total"><i class="fa fa-check"></i><b>2.3.3</b> Total</a></li>
<li class="chapter" data-level="2.3.4" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#mquadr"><i class="fa fa-check"></i><b>2.3.4</b> Média Quadrática</a></li>
<li class="chapter" data-level="2.3.5" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#moda"><i class="fa fa-check"></i><b>2.3.5</b> Moda</a></li>
<li class="chapter" data-level="2.3.6" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#separatrizes"><i class="fa fa-check"></i><b>2.3.6</b> Separatrizes</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#medidas-de-dispersão-ou-variabilidade"><i class="fa fa-check"></i><b>2.4</b> Medidas de Dispersão (ou Variabilidade)</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#amplitude"><i class="fa fa-check"></i><b>2.4.1</b> Amplitude</a></li>
<li class="chapter" data-level="2.4.2" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#variância"><i class="fa fa-check"></i><b>2.4.2</b> Variância</a></li>
<li class="chapter" data-level="2.4.3" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#desvio-padrão"><i class="fa fa-check"></i><b>2.4.3</b> Desvio Padrão</a></li>
<li class="chapter" data-level="2.4.4" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#coeficiente-de-variação"><i class="fa fa-check"></i><b>2.4.4</b> Coeficiente de variação</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#outras-medidas"><i class="fa fa-check"></i><b>2.5</b> Outras medidas</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#assimetria-ou-obliquidade"><i class="fa fa-check"></i><b>2.5.1</b> Assimetria (ou Obliquidade)</a></li>
<li class="chapter" data-level="2.5.2" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#curtose"><i class="fa fa-check"></i><b>2.5.2</b> Curtose</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#visualização"><i class="fa fa-check"></i><b>2.6</b> Visualização</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="estatistica-descritiva.html"><a href="estatistica-descritiva.html#menu-de-opções"><i class="fa fa-check"></i><b>2.6.1</b> Menu de opções</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probabilidade.html"><a href="probabilidade.html"><i class="fa fa-check"></i><b>3</b> Probabilidade</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probabilidade.html"><a href="probabilidade.html#propriedades"><i class="fa fa-check"></i><b>3.1</b> Propriedades</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="probabilidade.html"><a href="probabilidade.html#propriedades-fundamentais-axiomas-de-kolmogorov"><i class="fa fa-check"></i><b>3.1.1</b> Propriedades fundamentais (Axiomas de Kolmogorov)</a></li>
<li class="chapter" data-level="3.1.2" data-path="probabilidade.html"><a href="probabilidade.html#propriedades-secundárias"><i class="fa fa-check"></i><b>3.1.2</b> Propriedades secundárias</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="probabilidade.html"><a href="probabilidade.html#r-como-um-conjunto-de-tabelas-estatísticas"><i class="fa fa-check"></i><b>3.2</b> R como um conjunto de tabelas estatísticas</a></li>
<li class="chapter" data-level="3.3" data-path="probabilidade.html"><a href="probabilidade.html#distribuição-normal"><i class="fa fa-check"></i><b>3.3</b> Distribuição Normal</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="probabilidade.html"><a href="probabilidade.html#normal-univariada"><i class="fa fa-check"></i><b>3.3.1</b> Normal univariada</a></li>
<li class="chapter" data-level="3.3.2" data-path="probabilidade.html"><a href="probabilidade.html#normal-bivariada"><i class="fa fa-check"></i><b>3.3.2</b> Normal bivariada</a></li>
<li class="chapter" data-level="3.3.3" data-path="probabilidade.html"><a href="probabilidade.html#normal-multivariada"><i class="fa fa-check"></i><b>3.3.3</b> Normal multivariada</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="amostragem.html"><a href="amostragem.html"><i class="fa fa-check"></i><b>4</b> Amostragem</a>
<ul>
<li class="chapter" data-level="4.1" data-path="amostragem.html"><a href="amostragem.html#definições-básicas"><i class="fa fa-check"></i><b>4.1</b> Definições básicas</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="amostragem.html"><a href="amostragem.html#unidade-elementar"><i class="fa fa-check"></i><b>4.1.1</b> Unidade Elementar</a></li>
<li class="chapter" data-level="4.1.2" data-path="amostragem.html"><a href="amostragem.html#unidade-amostral"><i class="fa fa-check"></i><b>4.1.2</b> Unidade Amostral</a></li>
<li class="chapter" data-level="4.1.3" data-path="amostragem.html"><a href="amostragem.html#sistema-de-referências"><i class="fa fa-check"></i><b>4.1.3</b> Sistema de referências</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="amostragem.html"><a href="amostragem.html#universo-mathcalu"><i class="fa fa-check"></i><b>4.2</b> Universo <span class="math inline">\(\mathcal{U}\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="amostragem.html"><a href="amostragem.html#parâmetros"><i class="fa fa-check"></i><b>4.2.1</b> Parâmetros</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="amostragem.html"><a href="amostragem.html#amostras"><i class="fa fa-check"></i><b>4.3</b> Amostras</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="amostragem.html"><a href="amostragem.html#plano-amostral"><i class="fa fa-check"></i><b>4.3.1</b> Plano Amostral</a></li>
<li class="chapter" data-level="4.3.2" data-path="amostragem.html"><a href="amostragem.html#distribuições-amostrais"><i class="fa fa-check"></i><b>4.3.2</b> Distribuições amostrais</a></li>
<li class="chapter" data-level="4.3.3" data-path="amostragem.html"><a href="amostragem.html#amostra-representativa"><i class="fa fa-check"></i><b>4.3.3</b> Amostra representativa</a></li>
<li class="chapter" data-level="4.3.4" data-path="amostragem.html"><a href="amostragem.html#tipos-de-amostras"><i class="fa fa-check"></i><b>4.3.4</b> Tipos de amostras</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="amostragem.html"><a href="amostragem.html#principais-técnicas-de-amostragem"><i class="fa fa-check"></i><b>4.4</b> Principais técnicas de amostragem</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="amostragem.html"><a href="amostragem.html#amostragem-aleatória-simples"><i class="fa fa-check"></i><b>4.4.1</b> Amostragem Aleatória Simples</a></li>
<li class="chapter" data-level="4.4.2" data-path="amostragem.html"><a href="amostragem.html#amostragem-sistemática"><i class="fa fa-check"></i><b>4.4.2</b> Amostragem Sistemática</a></li>
<li class="chapter" data-level="4.4.3" data-path="amostragem.html"><a href="amostragem.html#amostragem-estratificada"><i class="fa fa-check"></i><b>4.4.3</b> Amostragem Estratificada</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="amostragem.html"><a href="amostragem.html#cálculo-do-tamanho-da-amostra"><i class="fa fa-check"></i><b>4.5</b> Cálculo do tamanho da amostra</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="amostragem.html"><a href="amostragem.html#média"><i class="fa fa-check"></i><b>4.5.1</b> Média</a></li>
<li class="chapter" data-level="4.5.2" data-path="amostragem.html"><a href="amostragem.html#proporção"><i class="fa fa-check"></i><b>4.5.2</b> Proporção</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="amostragem.html"><a href="amostragem.html#para-saber-mais"><i class="fa fa-check"></i><b>4.6</b> Para saber mais</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inferencia-classica.html"><a href="inferencia-classica.html"><i class="fa fa-check"></i><b>5</b> Inferência Clássica</a>
<ul>
<li class="chapter" data-level="5.1" data-path="inferencia-classica.html"><a href="inferencia-classica.html#estimação-pontual"><i class="fa fa-check"></i><b>5.1</b> Estimação Pontual</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="inferencia-classica.html"><a href="inferencia-classica.html#estimadores-e-suas-propriedades"><i class="fa fa-check"></i><b>5.1.1</b> Estimadores e suas propriedades</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="inferencia-classica.html"><a href="inferencia-classica.html#estimação-por-intervalo-de-confiança"><i class="fa fa-check"></i><b>5.2</b> (Estimação por) Intervalo de Confiança</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="inferencia-classica.html"><a href="inferencia-classica.html#média-1"><i class="fa fa-check"></i><b>5.2.1</b> Média</a></li>
<li class="chapter" data-level="5.2.2" data-path="inferencia-classica.html"><a href="inferencia-classica.html#proporção-1"><i class="fa fa-check"></i><b>5.2.2</b> Proporção</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inferencia-classica.html"><a href="inferencia-classica.html#estimação-por-teste-de-hipóteses"><i class="fa fa-check"></i><b>5.3</b> (Estimação por) Teste de Hipóteses</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="inferencia-classica.html"><a href="inferencia-classica.html#testes-paramétricos-univariados"><i class="fa fa-check"></i><b>5.3.1</b> Testes Paramétricos Univariados</a></li>
<li class="chapter" data-level="5.3.2" data-path="inferencia-classica.html"><a href="inferencia-classica.html#testes-paramétricos-bivariados"><i class="fa fa-check"></i><b>5.3.2</b> Testes Paramétricos Bivariados</a></li>
<li class="chapter" data-level="5.3.3" data-path="inferencia-classica.html"><a href="inferencia-classica.html#testes-paramétricos-multivariados"><i class="fa fa-check"></i><b>5.3.3</b> Testes Paramétricos Multivariados</a></li>
<li class="chapter" data-level="5.3.4" data-path="inferencia-classica.html"><a href="inferencia-classica.html#testes-não-paramétricos-univariados"><i class="fa fa-check"></i><b>5.3.4</b> Testes Não Paramétricos Univariados</a></li>
<li class="chapter" data-level="5.3.5" data-path="inferencia-classica.html"><a href="inferencia-classica.html#testes-não-paramétricos-bivariados"><i class="fa fa-check"></i><b>5.3.5</b> Testes Não Paramétricos Bivariados</a></li>
<li class="chapter" data-level="5.3.6" data-path="inferencia-classica.html"><a href="inferencia-classica.html#testes-não-paramétricos-multivariados"><i class="fa fa-check"></i><b>5.3.6</b> Testes Não Paramétricos Multivariados</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="inferencia-classica.html"><a href="inferencia-classica.html#exercícios"><i class="fa fa-check"></i><b>5.4</b> Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inferencia-bayesiana.html"><a href="inferencia-bayesiana.html"><i class="fa fa-check"></i><b>6</b> Inferência Bayesiana</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inferencia-bayesiana.html"><a href="inferencia-bayesiana.html#princípios-de-verossimilhança-suficiência-e-condicionalidade"><i class="fa fa-check"></i><b>6.1</b> Princípios de verossimilhança, suficiência e condicionalidade</a></li>
<li class="chapter" data-level="6.2" data-path="inferencia-bayesiana.html"><a href="inferencia-bayesiana.html#distribuição-a-priori"><i class="fa fa-check"></i><b>6.2</b> Distribuição a priori</a></li>
<li class="chapter" data-level="6.3" data-path="inferencia-bayesiana.html"><a href="inferencia-bayesiana.html#estimação-pontual-1"><i class="fa fa-check"></i><b>6.3</b> Estimação Pontual</a></li>
<li class="chapter" data-level="6.4" data-path="inferencia-bayesiana.html"><a href="inferencia-bayesiana.html#estimação-por-intervaloregiões-de-credibilidade"><i class="fa fa-check"></i><b>6.4</b> (Estimação por) Intervalo/Regiões de Credibilidade</a></li>
<li class="chapter" data-level="6.5" data-path="inferencia-bayesiana.html"><a href="inferencia-bayesiana.html#estimação-por-teste-de-hipóteses-1"><i class="fa fa-check"></i><b>6.5</b> (Estimação por) Teste de Hipóteses</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="inferencia-bayesiana.html"><a href="inferencia-bayesiana.html#fatores-de-bayes"><i class="fa fa-check"></i><b>6.5.1</b> Fatores de Bayes</a></li>
<li class="chapter" data-level="6.5.2" data-path="inferencia-bayesiana.html"><a href="inferencia-bayesiana.html#fbst---full-bayesian-significance-test"><i class="fa fa-check"></i><b>6.5.2</b> FBST - <em>Full Bayesian Significance Test</em></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modelos-lineares.html"><a href="modelos-lineares.html"><i class="fa fa-check"></i><b>7</b> Modelos Lineares</a>
<ul>
<li class="chapter" data-level="7.1" data-path="modelos-lineares.html"><a href="modelos-lineares.html#correlacao"><i class="fa fa-check"></i><b>7.1</b> Correlação</a></li>
<li class="chapter" data-level="7.2" data-path="modelos-lineares.html"><a href="modelos-lineares.html#regressão-linear-simples"><i class="fa fa-check"></i><b>7.2</b> Regressão Linear Simples</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="modelos-lineares.html"><a href="modelos-lineares.html#modelo"><i class="fa fa-check"></i><b>7.2.1</b> Modelo</a></li>
<li class="chapter" data-level="7.2.2" data-path="modelos-lineares.html"><a href="modelos-lineares.html#estimativas-dos-parâmetros"><i class="fa fa-check"></i><b>7.2.2</b> Estimativas dos parâmetros</a></li>
<li class="chapter" data-level="7.2.3" data-path="modelos-lineares.html"><a href="modelos-lineares.html#análise-de-diagnóstico"><i class="fa fa-check"></i><b>7.2.3</b> Análise de diagnóstico</a></li>
<li class="chapter" data-level="7.2.4" data-path="modelos-lineares.html"><a href="modelos-lineares.html#modelo-rpo"><i class="fa fa-check"></i><b>7.2.4</b> Modelo RPO</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="modelos-lineares.html"><a href="modelos-lineares.html#regressão-linear-múltipla"><i class="fa fa-check"></i><b>7.3</b> Regressão Linear Múltipla</a></li>
<li class="chapter" data-level="7.4" data-path="modelos-lineares.html"><a href="modelos-lineares.html#regressão-logística"><i class="fa fa-check"></i><b>7.4</b> Regressão Logística</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="modelos-lineares.html"><a href="modelos-lineares.html#variáveis-bináriasdicotômicas"><i class="fa fa-check"></i><b>7.4.1</b> Variáveis binárias/dicotômicas</a></li>
<li class="chapter" data-level="7.4.2" data-path="modelos-lineares.html"><a href="modelos-lineares.html#o-modelo-de-regressão-logística"><i class="fa fa-check"></i><b>7.4.2</b> O modelo de regressão logística</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="aprendizadodemaquina.html"><a href="aprendizadodemaquina.html"><i class="fa fa-check"></i><b>8</b> Aprendizado de Máquina</a>
<ul>
<li class="chapter" data-level="8.1" data-path="aprendizadodemaquina.html"><a href="aprendizadodemaquina.html#análise-de-componentes-principais-pca"><i class="fa fa-check"></i><b>8.1</b> Análise de Componentes Principais (<em>PCA</em>)</a></li>
<li class="chapter" data-level="8.2" data-path="aprendizadodemaquina.html"><a href="aprendizadodemaquina.html#técnicas-de-agrupamento"><i class="fa fa-check"></i><b>8.2</b> Técnicas de Agrupamento</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="aprendizadodemaquina.html"><a href="aprendizadodemaquina.html#medidas-de-similaridade-e-dissimilaridade"><i class="fa fa-check"></i><b>8.2.1</b> Medidas de similaridade e dissimilaridade</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="aprendizadodemaquina.html"><a href="aprendizadodemaquina.html#métodos-hierárquicos"><i class="fa fa-check"></i><b>8.3</b> Métodos hierárquicos</a></li>
<li class="chapter" data-level="8.4" data-path="aprendizadodemaquina.html"><a href="aprendizadodemaquina.html#métodos-não-hierárquicos-de-particionamento"><i class="fa fa-check"></i><b>8.4</b> Métodos não hierárquicos (de particionamento)</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="aprendizadodemaquina.html"><a href="aprendizadodemaquina.html#k-médias"><i class="fa fa-check"></i><b>8.4.1</b> K-médias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="seriestemporais.html"><a href="seriestemporais.html"><i class="fa fa-check"></i><b>9</b> Séries Temporais</a>
<ul>
<li class="chapter" data-level="9.1" data-path="seriestemporais.html"><a href="seriestemporais.html#impacto-causal"><i class="fa fa-check"></i><b>9.1</b> Impacto Causal</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="referências.html"><a href="referências.html"><i class="fa fa-check"></i><b>10</b> Referências</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de Estatística para Ciência de Dados</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="aprendizadodemaquina" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Aprendizado de Máquina</h1>
<p>A aplicação de <em>aprendizado de máquina</em> ou <em>modelagem algorítmica</em> está em crescente expansão. A aplicação deste tipo de metodologia usualmente utiliza <em>modelagem preditiva</em>, e não inferencial. Para maiores detalhes veja a discussão nas Seções 1.2 e 1.3 de <span class="citation">(Izbicki and Santos <a href="#ref-izbicki2020aprendizado" role="doc-biblioref">2020</a>)</span>, bem como nos slides disponibilizados pelo professor <a href="https://github.com/filipezabala/modelagem_preditiva">neste</a> link.</p>
<div id="análise-de-componentes-principais-pca" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Análise de Componentes Principais (<em>PCA</em>)</h2>
<p>A <em>Análise de Componentes Principais</em> (<em>PCA</em>, na sigla em inglês) é uma técnica de redução de dimensionalidade usualmente aplicada a um grande número de variáveis relacionadas, de forma a capturar o máximo possível da variabilidade do conjunto de dados. Foi introduzida por <span class="citation">(Pearson <a href="#ref-pearson1901on" role="doc-biblioref">1901</a>)</span> e estudada independentemente por <span class="citation">(Hotelling <a href="#ref-hotelling1933analysis" role="doc-biblioref">1933</a>)</span> e outros pesquisadores que abordaram o problema de formas variadas. Considerando a definição de <span class="citation">(Bishop <a href="#ref-bishop1999bayesian" role="doc-biblioref">1999</a>)</span>, seja um conjunto de dados <span class="math inline">\(X\)</span> de dimensão <span class="math inline">\(n \times p\)</span> composto por <span class="math inline">\(n\)</span> vetores <span class="math inline">\(p\)</span>-dimensionais conforme indicado a seguir. <span class="math display">\[ X = \begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} \\ x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} \end{bmatrix} = \begin{bmatrix} \boldsymbol{x}_{1}  \\ \boldsymbol{x}_{2} \\ \vdots \\ \boldsymbol{x}_{n} \end{bmatrix} \]</span></p>
<p>Deste conjunto de dados calcula-se a matriz de covariâncias amostrais <span class="math inline">\(\boldsymbol{S}\)</span> dada por
<span class="math display" id="eq:cov">\[\begin{equation}
\boldsymbol{S} = \dfrac{1}{n} \sum_{i=1}^{n} (\boldsymbol{x}_{i} - \bar{\boldsymbol{x}}) (\boldsymbol{x}_{i} - \bar{\boldsymbol{x}})^T,
\tag{8.1}
\end{equation}\]</span>
onde <span class="math inline">\(\bar{\boldsymbol{x}} = n^{-1} \sum_{i=1}^{n} \boldsymbol{x}_{i}\)</span> é o vetor de médias amostrais. São obtidos os autovetores <span class="math inline">\(\boldsymbol{u}_i\)</span> e os autovalores <span class="math inline">\(\lambda_i\)</span> de <span class="math inline">\(\boldsymbol{S}\)</span> pela equação
<span class="math display" id="eq:av">\[\begin{equation}
\boldsymbol{Su}_i = \lambda_i \boldsymbol{u}_i,
\tag{8.2}
\end{equation}\]</span>
<span class="math inline">\(i=1,\ldots,p\)</span>. Os autovetores correspondentes aos <span class="math inline">\(q\)</span> maiores autovetores (<span class="math inline">\(q&lt;p\)</span>) são mantidos, e uma representação de dimensão reduzida é definida por uma combinação linear dos autovetores e dos dados deslocados pela média. Matematicamente, <span class="math inline">\(d_n=\boldsymbol{U}^T(\boldsymbol{x}_n - \boldsymbol{\bar{x}})^T\)</span> onde <span class="math inline">\(\boldsymbol{U}_q=(\boldsymbol{u}_1,\ldots,\boldsymbol{u}_q)\)</span>.</p>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb627-1"><a href="aprendizadodemaquina.html#cb627-1"></a>df &lt;-<span class="st"> </span>iris[<span class="op">-</span><span class="dv">5</span>]      <span class="co"># retirando a quinta coluna, &#39;Species&#39;</span></span>
<span id="cb627-2"><a href="aprendizadodemaquina.html#cb627-2"></a>(m &lt;-<span class="st"> </span><span class="kw">colMeans</span>(df)) <span class="co"># vetor de médias</span></span></code></pre></div>
<pre><code>## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##         5.84         3.06         3.76         1.20</code></pre>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="aprendizadodemaquina.html#cb629-1"></a>(S &lt;-<span class="st"> </span><span class="kw">cov</span>(df))      <span class="co"># matriz de covariâncias</span></span></code></pre></div>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length       0.6857     -0.0424         1.27       0.516
## Sepal.Width       -0.0424      0.1900        -0.33      -0.122
## Petal.Length       1.2743     -0.3297         3.12       1.296
## Petal.Width        0.5163     -0.1216         1.30       0.581</code></pre>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="aprendizadodemaquina.html#cb631-1"></a><span class="kw">eigen</span>(S)            <span class="co"># autovalores (variâncias) e autovetores de S</span></span></code></pre></div>
<pre><code>## eigen() decomposition
## $values
## [1] 4.2282 0.2427 0.0782 0.0238
## 
## $vectors
##         [,1]    [,2]    [,3]   [,4]
## [1,]  0.3614 -0.6566 -0.5820  0.315
## [2,] -0.0845 -0.7302  0.5979 -0.320
## [3,]  0.8567  0.1734  0.0762 -0.480
## [4,]  0.3583  0.0755  0.5458  0.754</code></pre>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="aprendizadodemaquina.html#cb633-1"></a>(av &lt;-<span class="st"> </span><span class="kw">prcomp</span>(df))  <span class="co"># via função</span></span></code></pre></div>
<pre><code>## Standard deviations (1, .., p=4):
## [1] 2.056 0.493 0.280 0.154
## 
## Rotation (n x k) = (4 x 4):
##                  PC1     PC2     PC3    PC4
## Sepal.Length  0.3614 -0.6566  0.5820  0.315
## Sepal.Width  -0.0845 -0.7302 -0.5979 -0.320
## Petal.Length  0.8567  0.1734 -0.0762 -0.480
## Petal.Width   0.3583  0.0755 -0.5458  0.754</code></pre>
<p>É possível realizar o mesmo procedimento na matriz de correlação <span class="math inline">\(R\)</span>. Esta abordagem é recomendada para evitar que os resultados sejam afetados pela escala dos valores observados.</p>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb635-1"><a href="aprendizadodemaquina.html#cb635-1"></a>(R &lt;-<span class="st"> </span><span class="kw">cor</span>(df))  <span class="co"># matriz de correlação</span></span></code></pre></div>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length        1.000      -0.118        0.872       0.818
## Sepal.Width        -0.118       1.000       -0.428      -0.366
## Petal.Length        0.872      -0.428        1.000       0.963
## Petal.Width         0.818      -0.366        0.963       1.000</code></pre>
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb637-1"><a href="aprendizadodemaquina.html#cb637-1"></a><span class="kw">eigen</span>(R)        <span class="co"># autovalores e autovetores de R</span></span></code></pre></div>
<pre><code>## eigen() decomposition
## $values
## [1] 2.9185 0.9140 0.1468 0.0207
## 
## $vectors
##        [,1]    [,2]   [,3]   [,4]
## [1,]  0.521 -0.3774  0.720  0.261
## [2,] -0.269 -0.9233 -0.244 -0.124
## [3,]  0.580 -0.0245 -0.142 -0.801
## [4,]  0.565 -0.0669 -0.634  0.524</code></pre>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb639-1"><a href="aprendizadodemaquina.html#cb639-1"></a><span class="kw">prcomp</span>(df, <span class="dt">scale =</span> T)  <span class="co"># via função, scale = TRUE</span></span></code></pre></div>
<pre><code>## Standard deviations (1, .., p=4):
## [1] 1.708 0.956 0.383 0.144
## 
## Rotation (n x k) = (4 x 4):
##                 PC1     PC2    PC3    PC4
## Sepal.Length  0.521 -0.3774  0.720  0.261
## Sepal.Width  -0.269 -0.9233 -0.244 -0.124
## Petal.Length  0.580 -0.0245 -0.142 -0.801
## Petal.Width   0.565 -0.0669 -0.634  0.524</code></pre>
<p>A proporção da variância explicada pelo <span class="math inline">\(i\)</span>-ésimo componente principal é dada pela Eq. <span class="math inline">\(\eqref{eq:propvar}\)</span>, e pode ser visualizada em um gráfico ordenado, usualmente chamado <em>screeplot</em>.
<span class="math display" id="eq:propvar">\[\begin{equation}
PVE_i = \dfrac{\lambda_i}{\sum_{j=1}^{p} \lambda_j}
\tag{8.3}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb641"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb641-1"><a href="aprendizadodemaquina.html#cb641-1"></a>(vS &lt;-<span class="st"> </span><span class="kw">eigen</span>(S)<span class="op">$</span>values) <span class="co"># autovalores (variâncias) a partir de S</span></span></code></pre></div>
<pre><code>## [1] 4.2282 0.2427 0.0782 0.0238</code></pre>
<div class="sourceCode" id="cb643"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb643-1"><a href="aprendizadodemaquina.html#cb643-1"></a>vS<span class="op">/</span><span class="kw">sum</span>(vS)  <span class="co"># Equação (17)</span></span></code></pre></div>
<pre><code>## [1] 0.92462 0.05307 0.01710 0.00521</code></pre>
<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb645-1"><a href="aprendizadodemaquina.html#cb645-1"></a><span class="kw">screeplot</span>(<span class="kw">prcomp</span>(df), <span class="dt">type =</span> <span class="st">&#39;lines&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-145-1.png" width="672" /></p>
<div class="sourceCode" id="cb646"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb646-1"><a href="aprendizadodemaquina.html#cb646-1"></a>(vR &lt;-<span class="st"> </span><span class="kw">eigen</span>(R)<span class="op">$</span>values) <span class="co"># autovalores (variâncias) a partir de R</span></span></code></pre></div>
<pre><code>## [1] 2.9185 0.9140 0.1468 0.0207</code></pre>
<div class="sourceCode" id="cb648"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb648-1"><a href="aprendizadodemaquina.html#cb648-1"></a>vR<span class="op">/</span><span class="kw">sum</span>(vR)  <span class="co"># Equação (17)</span></span></code></pre></div>
<pre><code>## [1] 0.72962 0.22851 0.03669 0.00518</code></pre>
<div class="sourceCode" id="cb650"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb650-1"><a href="aprendizadodemaquina.html#cb650-1"></a><span class="kw">screeplot</span>(<span class="kw">prcomp</span>(df, <span class="dt">scale =</span> T), <span class="dt">type =</span> <span class="st">&#39;lines&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-145-2.png" width="672" /></p>
<p>Considere o banco de dados <code>iris</code>, que contém 4 colunas numéricas com as larguras e comprimentos das pétalas e sépalas de três espécies de flores do gênero íris.</p>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb651-1"><a href="aprendizadodemaquina.html#cb651-1"></a><span class="kw">head</span>(iris)</span></code></pre></div>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
<p>Existem <span class="math inline">\({4 \choose 2} = 6\)</span> combinações possíveis de gráficos bidimensionais, apresentados a seguir.</p>
<div class="sourceCode" id="cb653"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb653-1"><a href="aprendizadodemaquina.html#cb653-1"></a><span class="kw">require</span>(gridExtra)</span>
<span id="cb653-2"><a href="aprendizadodemaquina.html#cb653-2"></a>p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(iris, <span class="kw">aes</span>(Sepal.Length, Sepal.Width, <span class="dt">colour =</span> Species)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span>
<span id="cb653-3"><a href="aprendizadodemaquina.html#cb653-3"></a>p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(iris, <span class="kw">aes</span>(Sepal.Length, Petal.Length, <span class="dt">colour =</span> Species)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span>
<span id="cb653-4"><a href="aprendizadodemaquina.html#cb653-4"></a>p3 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(iris, <span class="kw">aes</span>(Sepal.Length, Petal.Width, <span class="dt">colour =</span> Species)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span>
<span id="cb653-5"><a href="aprendizadodemaquina.html#cb653-5"></a>p4 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(iris, <span class="kw">aes</span>(Sepal.Width, Petal.Length, <span class="dt">colour =</span> Species)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span>
<span id="cb653-6"><a href="aprendizadodemaquina.html#cb653-6"></a>p5 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(iris, <span class="kw">aes</span>(Sepal.Width, Petal.Width, <span class="dt">colour =</span> Species)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span>
<span id="cb653-7"><a href="aprendizadodemaquina.html#cb653-7"></a>p6 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(iris, <span class="kw">aes</span>(Petal.Length, Petal.Width, <span class="dt">colour =</span> Species)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span>
<span id="cb653-8"><a href="aprendizadodemaquina.html#cb653-8"></a><span class="kw">grid.arrange</span>(p1, p2, p3, p4, p5, p6, <span class="dt">ncol =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-147-1.png" width="672" /></p>
<p>É posível utilizar o método de componentes principais para aprimorar a visualização<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a> da estrutura de associação entre as diferentes espécies de plantas.</p>
<div class="sourceCode" id="cb654"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb654-1"><a href="aprendizadodemaquina.html#cb654-1"></a><span class="kw">library</span>(ggfortify)</span>
<span id="cb654-2"><a href="aprendizadodemaquina.html#cb654-2"></a><span class="kw">autoplot</span>(<span class="kw">prcomp</span>(df), <span class="dt">data =</span> iris, <span class="dt">colour =</span> <span class="st">&#39;Species&#39;</span>, <span class="dt">loadings =</span> T, <span class="dt">loadings.label =</span> T, <span class="dt">type =</span> <span class="st">&#39;raw&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-148-1.png" width="672" /></p>
<div class="sourceCode" id="cb655"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb655-1"><a href="aprendizadodemaquina.html#cb655-1"></a><span class="kw">autoplot</span>(<span class="kw">prcomp</span>(df, <span class="dt">scale =</span> T), <span class="dt">data =</span> iris, <span class="dt">colour =</span> <span class="st">&#39;Species&#39;</span>, <span class="dt">loadings =</span> T, <span class="dt">loadings.label =</span> T)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-148-2.png" width="672" /></p>

<div class="exercise">
<span id="exr:unnamed-chunk-149" class="exercise"><strong>Exercise 8.1  </strong></span>Considere o banco de dados sobre câncer de mama apresentado por <span class="citation">(Dua and Graff <a href="#ref-dua2019uci" role="doc-biblioref">2019</a>)</span>. A partir do código abaixo, faça uma anáise de componentes principais desconsiderando as duas primeiras colunas, que indicam respectivamente o código de identificação da paciente (<code>V1</code>) e o diagnóstico (<code>V2</code>, Benigno/Maligno).<br />
(a) Quais os valores de <span class="math inline">\(n\)</span> e <span class="math inline">\(p\)</span>?<br />
(b) O que ocorre no comando <code>x2 &lt;- x[,-c(1,2)]</code>?<br />
(c) Obtenha os autovalores e autovetores.<br />
(d) Apresente o <em>screeplot</em>.<br />
(e) Apresente o gráfico dos dois primeiros componentes principais colorido por <code>V2</code>.<br />
(f) Você considera que é possível associar os diagnósticos às variáveis <code>V3</code> a <code>V32</code>? Dica: observe se há algum tipo de agrupamento no gráfico do item (e).<br />
(g) Quais variáveis mais influenciam nos compomentes principais 1 e 2? Dica: use <code>loadings.label = T</code> na função <code>autoplot</code> do item (e) e observe o gráfico.
</div>
<div class="sourceCode" id="cb656"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb656-1"><a href="aprendizadodemaquina.html#cb656-1"></a><span class="kw">library</span>(ggfortify)</span>
<span id="cb656-2"><a href="aprendizadodemaquina.html#cb656-2"></a>x &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data&#39;</span>, <span class="dt">sep =</span> <span class="st">&#39;,&#39;</span>)</span>
<span id="cb656-3"><a href="aprendizadodemaquina.html#cb656-3"></a>x2 &lt;-<span class="st"> </span>x[,<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)]</span></code></pre></div>
</div>
<div id="técnicas-de-agrupamento" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Técnicas de Agrupamento</h2>
<p>Seguindo a definição de <span class="citation">(Hartigan <a href="#ref-hartigan1975clustering" role="doc-biblioref">1975</a>)</span>, <em>agrupamento</em> - <em>clustering</em> ou ainda <em>segmentação de dados</em> - é o <em>agrupamento de objetos similares</em>. Objetiva agregar observações que sejam similares em relação a características admitidas nos modelos considerados. Podem ser <em>aglomerativas</em>, quando definem uma delimitação ascendente - onde cada observação inicia como um grupo e se agrega com outras ao longo das iterações - ou <em>divisivas</em> se o cercamento é descendente - quando todas as observações começam em um grupo que vai sendo dividido a cada etapa.<br />
A linguagem R possui diversas funções para análise de agrupamento, sendo as principais discutidas neste capítulo. Para informações atualizadas, veja <a href="https://cran.r-project.org/web/views/Cluster.html" class="uri">https://cran.r-project.org/web/views/Cluster.html</a>.</p>
<div id="medidas-de-similaridade-e-dissimilaridade" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Medidas de similaridade e dissimilaridade</h3>
<!-- https://newonlinecourses.science.psu.edu/stat508/lesson/1b/1b.2/1b.2.1 -->
<p><em>Distâncias</em> e <em>divergências</em> são métricas utilizadas em problemas de classificação, agrupamento e reconhecimento de padrões. São utilizadas para medir a <em>similaridade</em> ou <em>dissimilaridade</em> entre pontos, vetores e distribuições. É comum realizar a <em>padronização</em>, i.e., subtrair cada valor da média e dividir pelo desvio padrão da coluna à qual o valor pertence. Este procedimento pode ser realizado através da função <code>base::scale</code>.</p>
<blockquote>
<p>Medida de similaridade avalia o quão similares são dois entes, ficando entre 0 (sem similariadade) e 1 (completamente similares).</p>
</blockquote>
<blockquote>
<p>Medida de dissimilaridade indica o quão distintos são dois entes, ficando entre 0 (iguais) e infinito (diferentes).</p>
</blockquote>
<p>As seguir são apresentadas algumas das principais distâncias da literatura – enquadradas na definição de medidas de dissimilaridade – e calculadas entre dois vetores <span class="math inline">\(\boldsymbol{x}\)</span> e <span class="math inline">\(\boldsymbol{y}\)</span>, usualmente linhas de uma matriz numérica. Apresenta-se ainda um pequeno banco de dados para a aplicação dos exemplos.</p>
<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb657-1"><a href="aprendizadodemaquina.html#cb657-1"></a><span class="co"># criando data frame &#39;df&#39; para os exemplos a seguir</span></span>
<span id="cb657-2"><a href="aprendizadodemaquina.html#cb657-2"></a>df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">V1=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">1</span>), <span class="dt">V2=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">4</span>))  <span class="co"># vetores V1 e V2</span></span>
<span id="cb657-3"><a href="aprendizadodemaquina.html#cb657-3"></a><span class="kw">rownames</span>(df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>)  <span class="co"># rótulo das linhas</span></span>
<span id="cb657-4"><a href="aprendizadodemaquina.html#cb657-4"></a>df</span></code></pre></div>
<pre><code>##   V1 V2
## x  3  2
## y  1  4</code></pre>
<div class="sourceCode" id="cb659"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb659-1"><a href="aprendizadodemaquina.html#cb659-1"></a><span class="co"># padronizando os dados</span></span>
<span id="cb659-2"><a href="aprendizadodemaquina.html#cb659-2"></a>df.s &lt;-<span class="st"> </span><span class="kw">scale</span>(df)</span>
<span id="cb659-3"><a href="aprendizadodemaquina.html#cb659-3"></a>df.s</span></code></pre></div>
<pre><code>##       V1     V2
## x  0.707 -0.707
## y -0.707  0.707
## attr(,&quot;scaled:center&quot;)
## V1 V2 
##  2  3 
## attr(,&quot;scaled:scale&quot;)
##   V1   V2 
## 1.41 1.41</code></pre>
<div id="distância-de-manhattan" class="section level4 unnumbered" number="">
<h4>Distância de Manhattan</h4>
<p>A <em>distância de Manhattan</em>, <em>norma 1</em> ou <span class="math inline">\(L_1\)</span> é uma medida de dissimilaridade que avalia a distância absoluta entre dois vetores, dada pela Equação <span class="math inline">\(\eqref{eq:manh}\)</span>.
<span class="math display" id="eq:manh">\[\begin{equation}
L_1 = \sum_{i=1}^{n} |x_i - y_i|
\tag{8.4}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb661"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb661-1"><a href="aprendizadodemaquina.html#cb661-1"></a><span class="kw">sum</span>(<span class="kw">abs</span>(df[<span class="dv">1</span>,]<span class="op">-</span>df[<span class="dv">2</span>,]))  <span class="co"># distância manhattan aplicando Eq. (18)</span></span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<div class="sourceCode" id="cb663"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb663-1"><a href="aprendizadodemaquina.html#cb663-1"></a><span class="kw">dist</span>(df, <span class="dt">method =</span> <span class="st">&#39;manhattan&#39;</span>)  <span class="co"># distância manhattan via &#39;dist&#39;</span></span></code></pre></div>
<pre><code>##   x
## y 4</code></pre>
<div class="sourceCode" id="cb665"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb665-1"><a href="aprendizadodemaquina.html#cb665-1"></a><span class="kw">dist</span>(df.s, <span class="dt">method =</span> <span class="st">&#39;manhattan&#39;</span>)  <span class="co"># distância manhattan via &#39;dist&#39; dos valores padronizados</span></span></code></pre></div>
<pre><code>##      x
## y 2.83</code></pre>
</div>
<div id="distância-euclidiana" class="section level4 unnumbered" number="">
<h4>Distância euclidiana</h4>
<p>Uma das mais utilizadas medidas de dissimilaridade da literatura, a <em>distância euclidiana</em>, <em>norma 2</em> ou <span class="math inline">\(L_2\)</span> é dada pela Equação <span class="math inline">\(\eqref{eq:eucl}\)</span>.
<span class="math display" id="eq:eucl">\[\begin{equation}
L_2 = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
\tag{8.5}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb667"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb667-1"><a href="aprendizadodemaquina.html#cb667-1"></a><span class="kw">sqrt</span>(<span class="kw">sum</span>((df[<span class="dv">1</span>,]<span class="op">-</span>df[<span class="dv">2</span>,])<span class="op">^</span><span class="dv">2</span>))  <span class="co"># distância euclidiana aplicando Eq. (19)</span></span></code></pre></div>
<pre><code>## [1] 2.83</code></pre>
<div class="sourceCode" id="cb669"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb669-1"><a href="aprendizadodemaquina.html#cb669-1"></a><span class="kw">dist</span>(df, <span class="dt">method =</span> <span class="st">&#39;euclidean&#39;</span>)  <span class="co"># distância euclidiana via &#39;dist&#39;</span></span></code></pre></div>
<pre><code>##      x
## y 2.83</code></pre>
<div class="sourceCode" id="cb671"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb671-1"><a href="aprendizadodemaquina.html#cb671-1"></a><span class="kw">dist</span>(df.s, <span class="dt">method =</span> <span class="st">&#39;euclidean&#39;</span>)  <span class="co"># distância euclidiana via &#39;dist&#39; dos valores padronizados</span></span></code></pre></div>
<pre><code>##   x
## y 2</code></pre>
</div>
<div id="distância-de-minkowski" class="section level4 unnumbered" number="">
<h4>Distância de Minkowski</h4>
<p>A <em>distância de Minkowski</em>, <em>norma p</em> ou <span class="math inline">\(L_p\)</span> é uma medida de dissimilaridade que generaliza as distâncias de Manhattan e euclidiana. É dada pela Equação <span class="math inline">\(\eqref{eq:mink}\)</span>.
<span class="math display" id="eq:mink">\[\begin{equation}
L_p = \sqrt[\leftroot{-2}\uproot{3}p]{\sum_{i=1}^{n} (|x_i - y_i|)^p}
\tag{8.6}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb673"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb673-1"><a href="aprendizadodemaquina.html#cb673-1"></a><span class="kw">sum</span>((<span class="kw">abs</span>(df[<span class="dv">1</span>,]<span class="op">-</span>df[<span class="dv">2</span>,]))<span class="op">^</span><span class="dv">5</span>)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">5</span>)     <span class="co"># distância de Minkowski com p=5 aplicando Eq. (20)</span></span></code></pre></div>
<pre><code>## [1] 2.3</code></pre>
<div class="sourceCode" id="cb675"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb675-1"><a href="aprendizadodemaquina.html#cb675-1"></a><span class="kw">dist</span>(df, <span class="dt">method =</span> <span class="st">&#39;minkowski&#39;</span>, <span class="dt">p =</span> <span class="dv">5</span>) <span class="co"># distância de Minkowski com p=5 via &#39;dist&#39;</span></span></code></pre></div>
<pre><code>##     x
## y 2.3</code></pre>
<div class="sourceCode" id="cb677"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb677-1"><a href="aprendizadodemaquina.html#cb677-1"></a><span class="kw">dist</span>(df.s, <span class="dt">method =</span> <span class="st">&#39;minkowski&#39;</span>, <span class="dt">p =</span> <span class="dv">5</span>) <span class="co"># dist. de Minkowski com p=5 via &#39;dist&#39; dos valores padronizados</span></span></code></pre></div>
<pre><code>##      x
## y 1.62</code></pre>

<div class="exercise">
<span id="exr:unnamed-chunk-155" class="exercise"><strong>Exercise 8.2  </strong></span>Considere a função <code>stats::dist</code>.<br />
(a) Verifique sua documentação, fazendo <code>?dist</code>.<br />
(b) Compare as distâncias euclidiana e de Minkowski com <span class="math inline">\(p=2\)</span>. O que você observa?<br />
(c) Compare as distâncias de Manhattan e de Minkowski com <span class="math inline">\(p=1\)</span>. O que você observa?
</div>

<div class="exercise">
<span id="exr:unnamed-chunk-156" class="exercise"><strong>Exercise 8.3  </strong></span>Considere as distâncias da Seção 7.1 aplicada às colunas numéricas do banco de dados <code>pib</code>, obtido pelo código abaixo.<br />
(a) Padronize os dados e atribua a uma variável chamada <code>pib.s</code>.<br />
(b) Realize os cálculos ‘a mão’ como nos exemplos, tanto para <code>pib</code> quanto para <code>pib.s</code>.<br />
(c) Realize novamente os cálculos do item (b) utilizando a função <code>dist</code>.
</div>
<div class="sourceCode" id="cb679"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb679-1"><a href="aprendizadodemaquina.html#cb679-1"></a>pib &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&#39;http://www.filipezabala.com/data/pib.txt&#39;</span>, <span class="dt">head =</span> T, <span class="dt">sep =</span> <span class="st">&#39;</span><span class="ch">\t</span><span class="st">&#39;</span>)</span></code></pre></div>
</div>
</div>
</div>
<div id="métodos-hierárquicos" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Métodos hierárquicos</h2>
<p>Como o nome sugere, os <em>métodos hierárquicos</em> buscam uma estrutura hierárquica dos grupos. Esta estrutura geralmente se dá em forma de árvore, onde os objetos são apresentados individualmente como um grupo unitário (folha/<em>leaf</em>) que se aglomeram por similaridade em grupos maiores (nós/<em>nodes</em>) ligados por um grande grupo que une todos elementos (raiz/<em>root</em>). Os passos para realizar um agrupamento hierárquico aglomerativo estão descritos a seguir.</p>
<p><strong>ALGORITMO DE AGRUPAMENTO HIERÁRQUICO AGLOMERATIVO</strong></p>
<blockquote>
<p><strong>PASSO 1</strong> Padronizar os dados, geralmente com o uso da função <code>scale</code>.</p>
</blockquote>
<blockquote>
<p><strong>PASSO 2</strong> Calcular a (dis)similaridade entre cada par de objetos no conjunto de dados.</p>
</blockquote>
<blockquote>
<p><strong>PASSO 3</strong> Usar a função de ligação para agrupar os objetos na árvore a partir das informações de distância obtidas na passo 1.</p>
</blockquote>
<blockquote>
<p><strong>PASSO 4</strong> Apresentar o gráfico da árvore hierárquica em grupos (dendograma), criando uma partição dos dados.</p>
</blockquote>
<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb680-1"><a href="aprendizadodemaquina.html#cb680-1"></a><span class="co"># 1. padronizando os dados</span></span>
<span id="cb680-2"><a href="aprendizadodemaquina.html#cb680-2"></a>USArrest.scale &lt;-<span class="st"> </span><span class="kw">scale</span>(USArrests)</span>
<span id="cb680-3"><a href="aprendizadodemaquina.html#cb680-3"></a><span class="co"># 2. calculando distâncias (utilizando o padrão: euclidean)</span></span>
<span id="cb680-4"><a href="aprendizadodemaquina.html#cb680-4"></a>dUSA &lt;-<span class="st"> </span><span class="kw">dist</span>(USArrest.scale)</span>
<span id="cb680-5"><a href="aprendizadodemaquina.html#cb680-5"></a><span class="co"># 3. aplicando a função de ligação (utilizando o padrão: complete)</span></span>
<span id="cb680-6"><a href="aprendizadodemaquina.html#cb680-6"></a>hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(dUSA)</span>
<span id="cb680-7"><a href="aprendizadodemaquina.html#cb680-7"></a><span class="co"># 4. apresentando o gráfico</span></span>
<span id="cb680-8"><a href="aprendizadodemaquina.html#cb680-8"></a><span class="kw">plot</span>(hc)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-158-1.png" width="672" /></p>
<p>Os valores no eixo <span class="math inline">\(y\)</span>, intitulados <em>height</em>, são as chamadas <em>distâncias cofenéticas</em> propostas por <span class="citation">(Sokal and Rohlf <a href="#ref-sokal1962comparison" role="doc-biblioref">1962</a>)</span>. O nome vem da área da Biologia chamada <a href="https://pt.wikipedia.org/wiki/Fen%C3%A9tica">fenética</a>, que estuda métodos de classificação por similaridade fenotípica. Seu cálculo não é complexo, mas pode ser trabalhoso; assim, será considerada a função <code>cophenetic</code> para a obtenção de tais distâncias. Quanto maior for seu valor, mais dissimilar são os elementos comparados. Correlação elevada entre as distâncias calculadas e as distâncias cofenéticas sugere um bom agrupamento.</p>
<div class="sourceCode" id="cb681"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb681-1"><a href="aprendizadodemaquina.html#cb681-1"></a><span class="co"># Calcula a matriz de distâncias cofenéticas</span></span>
<span id="cb681-2"><a href="aprendizadodemaquina.html#cb681-2"></a>coph &lt;-<span class="st"> </span><span class="kw">cophenetic</span>(hc)</span>
<span id="cb681-3"><a href="aprendizadodemaquina.html#cb681-3"></a><span class="kw">sort</span>(<span class="kw">unique</span>(coph))</span></code></pre></div>
<pre><code>##  [1] 0.206 0.350 0.429 0.494 0.530 0.535 0.594 0.646 0.704 0.711 0.739 0.772 0.778 0.787 0.798 0.829 0.841 0.846 0.982 0.997
## [21] 1.012 1.035 1.071 1.080 1.092 1.131 1.183 1.197 1.212 1.250 1.272 1.333 1.399 1.467 1.623 1.645 1.659 1.854 1.865 2.263
## [41] 2.295 2.337 2.446 2.475 3.088 3.255 4.401 4.420 6.077</code></pre>
<div class="sourceCode" id="cb683"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb683-1"><a href="aprendizadodemaquina.html#cb683-1"></a><span class="co"># Correlação entre as distâncias cofenéticas e as distâncias originais (maior, melhor)</span></span>
<span id="cb683-2"><a href="aprendizadodemaquina.html#cb683-2"></a><span class="kw">cor</span>(coph,dUSA)</span></code></pre></div>
<pre><code>## [1] 0.698</code></pre>
<p>É possível melhorar a visualização do dendograma utilizando a função <code>factoextra::fviz_dend</code>.</p>
<div class="sourceCode" id="cb685"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb685-1"><a href="aprendizadodemaquina.html#cb685-1"></a><span class="kw">library</span>(factoextra)</span>
<span id="cb685-2"><a href="aprendizadodemaquina.html#cb685-2"></a><span class="kw">fviz_dend</span>(hc, <span class="dt">cex =</span> <span class="fl">0.6</span>)  <span class="co"># fonte com 60% do tamanho</span></span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-160-1.png" width="672" /></p>
<p>Pode-se utilizar a função <code>fviz_dend</code> para colorir um número arbitrário de grupos. Note que os grupamentos são obtidos de cima pra baixo, dependente dos valores de <em>height</em> (distâncias cofenéticas).</p>
<div class="sourceCode" id="cb686"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb686-1"><a href="aprendizadodemaquina.html#cb686-1"></a><span class="kw">fviz_dend</span>(hc, <span class="dt">k =</span> <span class="dv">2</span>,  <span class="co"># 2 grupos</span></span>
<span id="cb686-2"><a href="aprendizadodemaquina.html#cb686-2"></a>          <span class="dt">cex =</span> <span class="fl">0.6</span>,  <span class="co"># tamanho do texto/rótulo (label)</span></span>
<span id="cb686-3"><a href="aprendizadodemaquina.html#cb686-3"></a>          <span class="dt">rect =</span> <span class="ot">TRUE</span> <span class="co"># adiciona retângulos ao redor dos grupos</span></span>
<span id="cb686-4"><a href="aprendizadodemaquina.html#cb686-4"></a>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-161-1.png" width="672" /></p>
<div class="sourceCode" id="cb687"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb687-1"><a href="aprendizadodemaquina.html#cb687-1"></a><span class="kw">fviz_dend</span>(hc, <span class="dt">k =</span> <span class="dv">3</span>,  <span class="co"># 3 grupos</span></span>
<span id="cb687-2"><a href="aprendizadodemaquina.html#cb687-2"></a>          <span class="dt">cex =</span> <span class="fl">0.6</span>,  <span class="co"># tamanho do texto/rótulo (label)</span></span>
<span id="cb687-3"><a href="aprendizadodemaquina.html#cb687-3"></a>          <span class="dt">rect =</span> <span class="ot">TRUE</span> <span class="co"># adiciona retângulos ao redor dos grupos</span></span>
<span id="cb687-4"><a href="aprendizadodemaquina.html#cb687-4"></a>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-162-1.png" width="672" /></p>
<div class="sourceCode" id="cb688"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb688-1"><a href="aprendizadodemaquina.html#cb688-1"></a><span class="kw">fviz_dend</span>(hc, <span class="dt">k =</span> <span class="dv">4</span>,  <span class="co"># 4 grupos</span></span>
<span id="cb688-2"><a href="aprendizadodemaquina.html#cb688-2"></a>          <span class="dt">cex =</span> <span class="fl">0.6</span>,  <span class="co"># tamanho do texto/rótulo (label)</span></span>
<span id="cb688-3"><a href="aprendizadodemaquina.html#cb688-3"></a>          <span class="dt">rect =</span> <span class="ot">TRUE</span> <span class="co"># adiciona retângulos ao redor dos grupos</span></span>
<span id="cb688-4"><a href="aprendizadodemaquina.html#cb688-4"></a>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-163-1.png" width="672" /></p>

<div class="exercise">
<span id="exr:unnamed-chunk-164" class="exercise"><strong>Exercise 8.4  </strong></span>Considere novamente o conjunto de dados <code>pib</code>. Crie dendogramas com a função <code>fviz_dend</code> utilizando:<br />
(a) dados originais e padronizados;<br />
(b) diferentes distâncias;<br />
(c) diferentes números de grupos.
</div>
</div>
<div id="métodos-não-hierárquicos-de-particionamento" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Métodos não hierárquicos (de particionamento)</h2>
<div id="k-médias" class="section level3" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> K-médias</h3>
<!-- https://rpubs.com/shirokaner/320218 -->
<!-- https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/ -->
<p>K-médias (<em>k-means</em>) é um nome genérico para métodos derivados dos algoritmos de <span class="citation">(Lloyd <a href="#ref-lloyd1982least" role="doc-biblioref">1957</a>)</span>, <span class="citation">(Forgy <a href="#ref-forgy1965cluster" role="doc-biblioref">1965</a>)</span>, <span class="citation">(MacQueen and others <a href="#ref-macqueen1967some" role="doc-biblioref">1967</a>)</span>, <span class="citation">(Hartigan <a href="#ref-hartigan1975clustering" role="doc-biblioref">1975</a>)</span> e <span class="citation">(Hartigan and Wong <a href="#ref-hartigan1979algorithm" role="doc-biblioref">1979</a>)</span>. A ideia básica é encontrar grupos similares, de maneira a minimizar a soma de distâncias euclidianas ao quadrado. As distâncias são calculadas entre os pontos e as médias de cada um dos <span class="math inline">\(k\)</span> grupos, chamadas <em>centróides</em>.</p>
<p>Em relação ao modo de busca podem ser classificados como algoritmos de <em>comutação</em>, em que objetos devem ser particionados em <span class="math inline">\(k\)</span> de grupos. Uma partição inicial é dada de forma arbitrária, onde se definem <span class="math inline">\(k\)</span> centróides. Calcula-se a distância euclidiana ao quadrado entre as observações e os <span class="math inline">\(k\)</span> centróides. O centróide mais próximo define o grupo ao qual uma observação pertence. Recalculam-se os novos centróides, e novas partições são obtidas com a alternância dos objetos entre os grupos. O algoritmo encerra quando nenhuma comutação adicional reduz a soma de quadrados intra-grupo, ou quando outro critério de parada é atingido.</p>
<p>São algoritmos relativamente rápidos na execução, mas são afetados pela incerteza da partição inicial. Há sempre a possibilidade de que partições iniciais distintas possam levar a partições finais superiores a outras.</p>
<p>A variação quadrática intra-grupo (<span class="math inline">\(VQI_{j}\)</span>) do <span class="math inline">\(j\)</span>-ésimo grupo é dada pela Equação <span class="math inline">\(\eqref{eq:vqi}\)</span>.
<span class="math display" id="eq:vqi">\[\begin{equation}
VQI_{j} = \sum_{x_i \in G_j} (x_i - \mu_j)^2
\tag{8.7}
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(x_i\)</span> é o <span class="math inline">\(i\)</span>-ésimo elemento pertencente ao grupo <span class="math inline">\(G_j\)</span><br />
</li>
<li><span class="math inline">\(\mu_j\)</span> é o ponto médio do grupo <span class="math inline">\(G_j\)</span><br />
</li>
<li><span class="math inline">\(j \in \{2, \ldots, k\}\)</span></li>
</ul>
<p>A soma de quadrados total (<span class="math inline">\(SQT\)</span>) é dada pela Equação <span class="math inline">\(\eqref{eq:sqt}\)</span>.
<span class="math display" id="eq:sqt">\[\begin{equation}
SQT = \sum_{i=1}^{k} VQI_{i}
\tag{8.8}
\end{equation}\]</span></p>
<p>Cada observação <span class="math inline">\(x_i\)</span> é atribuída a um grupo de forma que a <span class="math inline">\(SQT\)</span> seja mínima a cada iteração. É recomendado que seja feita a padronização dos dados, de maneira a controlar o impacto da escala na definição dos grupos.</p>
<p><strong>ALGORITMO DAS K-MÉDIAS</strong></p>
<blockquote>
<p><strong>PASSO 1</strong> Especifique o número <span class="math inline">\(k\)</span> de grupos a serem criados.</p>
</blockquote>
<blockquote>
<p><strong>PASSO 2</strong> Selecione arbitrariamente <span class="math inline">\(k\)</span> pontos como centros dos grupos (centróides).</p>
</blockquote>
<blockquote>
<p><strong>PASSO 3</strong> Atribua cada observação ao grupo de centróide mais próximo, baseado na distância euclidiana entre a observação e os centróides.</p>
</blockquote>
<blockquote>
<p><strong>PASSO 4</strong> Recalcule os centróides com os pontos atribuídos a cada grupo. O centróide do <span class="math inline">\(j\)</span>-ésimo grupo é um vetor de comprimento <span class="math inline">\(p\)</span> contendo as médias das <span class="math inline">\(p\)</span> variáveis, calculadas com todos os pontos atribuídos ao <span class="math inline">\(j\)</span>-ésimo grupo.</p>
</blockquote>
<blockquote>
<p><strong>PASSO 5</strong> Repita os passos 3 e 4 até que as atribuições não mais reduzam a soma de quadrados intra-grupo, ou que o número máximo de iterações (ou qualquer outro critério de parada) seja atingido.</p>
</blockquote>
<div id="seleção-inicial-dos-centróides" class="section level4 unnumbered" number="">
<h4>Seleção inicial dos centróides</h4>
<p><span class="citation">(Hartigan <a href="#ref-hartigan1975clustering" role="doc-biblioref">1975</a>)</span> sugere que a seleção inicial dos centróides seja baseada na soma dos casos <span class="math inline">\(S\)</span>, que tem um valor mínimo <span class="math inline">\(minS\)</span> e um máximo <span class="math inline">\(maxS\)</span>. Para obter <span class="math inline">\(k\)</span> grupos iniciais, propõe atribuir o <span class="math inline">\(i\)</span>-ésimo caso ao <span class="math inline">\(j\)</span>-ésimo grupo, onde <span class="math inline">\(j\)</span> é a parte inteira de
<span class="math display" id="eq:j">\[\begin{equation}
k \left( \dfrac{S-minS}{maxS-minS} \right) + 1
\tag{8.9}
\end{equation}\]</span>
Uma adaptação será feita, multiplicando 1.01 a <span class="math inline">\(maxS\)</span> para evitar encontrar <span class="math inline">\(j&gt;k\)</span>.</p>
<div class="sourceCode" id="cb689"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb689-1"><a href="aprendizadodemaquina.html#cb689-1"></a>iris2 &lt;-<span class="st"> </span><span class="kw">scale</span>(iris[<span class="op">-</span><span class="dv">5</span>])</span>
<span id="cb689-2"><a href="aprendizadodemaquina.html#cb689-2"></a>S &lt;-<span class="st"> </span><span class="kw">rowSums</span>(iris2)</span>
<span id="cb689-3"><a href="aprendizadodemaquina.html#cb689-3"></a>k &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb689-4"><a href="aprendizadodemaquina.html#cb689-4"></a>zab &lt;-<span class="st"> </span>k<span class="op">*</span>(S <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(S))<span class="op">/</span>(<span class="fl">1.01</span><span class="op">*</span><span class="kw">max</span>(S)<span class="op">-</span><span class="kw">min</span>(S)) <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="co"># atribuição de Zabala (2019) baseada em Hartigan (1975)</span></span>
<span id="cb689-5"><a href="aprendizadodemaquina.html#cb689-5"></a>(g &lt;-<span class="st"> </span><span class="kw">floor</span>(zab)) <span class="co"># grupos</span></span></code></pre></div>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 2 1 2 1 2
##  [60] 1 1 2 1 2 1 2 1 1 1 1 2 1 1 1 2 2 2 2 2 1 1 1 1 2 1 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2
## [119] 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
<div class="sourceCode" id="cb691"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb691-1"><a href="aprendizadodemaquina.html#cb691-1"></a><span class="kw">table</span>(g)</span></code></pre></div>
<pre><code>## g
##  1  2 
## 83 67</code></pre>
<div class="sourceCode" id="cb693"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb693-1"><a href="aprendizadodemaquina.html#cb693-1"></a>(centroide &lt;-<span class="st"> </span><span class="kw">by</span>(iris2, g, colMeans))</span></code></pre></div>
<pre><code>## INDICES: 1
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##      -0.7071       0.0398      -0.6881      -0.7025 
## --------------------------------------------------------------------------------------------- 
## INDICES: 2
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##       0.8759      -0.0494       0.8524       0.8703</code></pre>
<div class="sourceCode" id="cb695"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb695-1"><a href="aprendizadodemaquina.html#cb695-1"></a><span class="kw">summary</span>(iris2)</span></code></pre></div>
<pre><code>##   Sepal.Length     Sepal.Width      Petal.Length     Petal.Width    
##  Min.   :-1.864   Min.   :-2.426   Min.   :-1.562   Min.   :-1.442  
##  1st Qu.:-0.898   1st Qu.:-0.590   1st Qu.:-1.222   1st Qu.:-1.180  
##  Median :-0.052   Median :-0.132   Median : 0.335   Median : 0.132  
##  Mean   : 0.000   Mean   : 0.000   Mean   : 0.000   Mean   : 0.000  
##  3rd Qu.: 0.672   3rd Qu.: 0.557   3rd Qu.: 0.760   3rd Qu.: 0.788  
##  Max.   : 2.484   Max.   : 3.080   Max.   : 1.780   Max.   : 1.706</code></pre>

<div class="exercise">
<span id="exr:unnamed-chunk-166" class="exercise"><strong>Exercise 8.5  </strong></span>Utilize a Equação (23) sem a correção de Zabala e observe o resultado em <code>iris2</code>.
</div>

<div class="exercise">
<span id="exr:unnamed-chunk-167" class="exercise"><strong>Exercise 8.6  </strong></span>Utilize a Equação (23) com a correção de Zabala para criar uma função que defina os centróides iniciais para um valor genérico <span class="math inline">\(k\)</span> de grupos em uma base de dados numérica. Teste em <code>iris2</code> e outros bancos de dados já trabalhados.
</div>
</div>
<div id="implementando-no-r" class="section level4 unnumbered" number="">
<h4>Implementando no R</h4>
<p>No R pode-se utilizar a função <code>stats::kmeans</code> para definir os grupamentos através das k-médias. Por padrão, esta função utiliza 10 como valor padrão para o número máximo de iterações e inicia com <span class="math inline">\(k\)</span> centróides aleatórios.</p>
<div class="sourceCode" id="cb697"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb697-1"><a href="aprendizadodemaquina.html#cb697-1"></a>km &lt;-<span class="st"> </span><span class="cf">function</span>(dados,grupos){</span>
<span id="cb697-2"><a href="aprendizadodemaquina.html#cb697-2"></a>  k &lt;-<span class="st"> </span><span class="kw">kmeans</span>(dados,grupos)</span>
<span id="cb697-3"><a href="aprendizadodemaquina.html#cb697-3"></a>  <span class="kw">print</span>(<span class="kw">table</span>(iris<span class="op">$</span>Species, k<span class="op">$</span>cluster))</span>
<span id="cb697-4"><a href="aprendizadodemaquina.html#cb697-4"></a>  <span class="kw">plot</span>(dados, <span class="dt">col=</span>k<span class="op">$</span>cluster)</span>
<span id="cb697-5"><a href="aprendizadodemaquina.html#cb697-5"></a>}</span>
<span id="cb697-6"><a href="aprendizadodemaquina.html#cb697-6"></a><span class="kw">km</span>(iris2,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##             
##               1  2
##   setosa     50  0
##   versicolor  0 50
##   virginica   0 50</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-168-1.png" width="672" /></p>
<div class="sourceCode" id="cb699"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb699-1"><a href="aprendizadodemaquina.html#cb699-1"></a><span class="kw">km</span>(iris2,<span class="dv">3</span>)</span></code></pre></div>
<pre><code>##             
##               1  2  3
##   setosa      0 33 17
##   versicolor 46  0  4
##   virginica  50  0  0</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-168-2.png" width="672" /></p>
<div class="sourceCode" id="cb701"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb701-1"><a href="aprendizadodemaquina.html#cb701-1"></a><span class="kw">km</span>(iris2,<span class="dv">4</span>)</span></code></pre></div>
<pre><code>##             
##               1  2  3  4
##   setosa      0 16 34  0
##   versicolor 11  0  0 39
##   virginica  36  0  0 14</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-168-3.png" width="672" /></p>
<p>O pacote <code>factoextra</code> fornece uma série de melhorias para a análise de k-means. Além de gráficos mais sofisticados utilizando <code>ggplot2</code>, associa métodos hierárquicos e métodos de particionamento.</p>
<div class="sourceCode" id="cb703"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb703-1"><a href="aprendizadodemaquina.html#cb703-1"></a>km2 &lt;-<span class="st"> </span><span class="cf">function</span>(dados,grupos){</span>
<span id="cb703-2"><a href="aprendizadodemaquina.html#cb703-2"></a>  k &lt;-<span class="st"> </span><span class="kw">kmeans</span>(dados,grupos)</span>
<span id="cb703-3"><a href="aprendizadodemaquina.html#cb703-3"></a>  <span class="kw">fviz_cluster</span>(k, iris2, <span class="dt">repel =</span> T)</span>
<span id="cb703-4"><a href="aprendizadodemaquina.html#cb703-4"></a>}</span>
<span id="cb703-5"><a href="aprendizadodemaquina.html#cb703-5"></a><span class="kw">km2</span>(iris2,<span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-169-1.png" width="672" /></p>
<div class="sourceCode" id="cb704"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb704-1"><a href="aprendizadodemaquina.html#cb704-1"></a><span class="kw">km2</span>(iris2,<span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-169-2.png" width="672" /></p>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb705-1"><a href="aprendizadodemaquina.html#cb705-1"></a><span class="kw">km2</span>(iris2,<span class="dv">4</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-169-3.png" width="672" /></p>
</div>
<div id="número-de-grupos" class="section level4 unnumbered" number="">
<h4>Número de grupos</h4>
<p>A função <code>factoextra::fviz_nbclust</code> fornece métodos para a escolha de um número ótimo de grupos. O método <code>wss</code> (<em>total <strong>w</strong>ithin <strong>s</strong>um of <strong>s</strong>quare</em>), busca um número de grupos que traga um bom custo-benefício entre o número de grupos (<span class="math inline">\(k\)</span>) e a soma de quadrados total (<span class="math inline">\(SQT\)</span>). Este custo-benefício é indicado onde a curva muda sua declividade, ou no ‘cotovelo’ (<em>elbow</em>) do gráfico de <span class="math inline">\(k\)</span> por <span class="math inline">\(SQT\)</span>. Tem suas origens no trabalho de <span class="citation">(Thorndike <a href="#ref-thorndike1953belongs" role="doc-biblioref">1953</a>)</span>.</p>
<div class="sourceCode" id="cb706"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb706-1"><a href="aprendizadodemaquina.html#cb706-1"></a><span class="kw">fviz_nbclust</span>(iris2, kmeans, <span class="dt">method =</span> <span class="st">&#39;wss&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-170-1.png" width="672" /></p>
<p>O método <code>silhouette</code> busca o número de grupos que maximize o tamanho médio da silhueta. É baseado em uma medida sugerida por <span class="citation">(Rousseeuw and Kaufman <a href="#ref-rousseeuw1990finding" role="doc-biblioref">1990</a>)</span>, dada por
<span class="math display" id="eq:silhueta">\[\begin{equation}
s(i) = \dfrac{b(i)-a(i)}{max\{ a(i),b(i) \}}
\tag{8.10}
\end{equation}\]</span>
- <span class="math inline">\(-1 \le s(i) \le 1\)</span><br />
- <span class="math inline">\(a(i)\)</span>: dissimilaridade média do elemento <span class="math inline">\(i\)</span> em relação a todos os demais elementos do seu grupo <span class="math inline">\(A\)</span><br />
- <span class="math inline">\(d(i,C)\)</span>: dissimilaridade média do elemento <span class="math inline">\(i\)</span> em relação a todos os elementos do grupo <span class="math inline">\(C \ne A\)</span><br />
- <span class="math inline">\(b(i) = \underset{C \ne A}{\mathrm{min}} \; d(i,C)\)</span></p>
<div class="sourceCode" id="cb707"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb707-1"><a href="aprendizadodemaquina.html#cb707-1"></a><span class="kw">fviz_nbclust</span>(iris2, kmeans, <span class="dt">method =</span> <span class="st">&#39;silhouette&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-171-1.png" width="672" /></p>
<p>Proposto por <span class="citation">(Tibshirani, Walther, and Hastie <a href="#ref-tibshirani2001estimating" role="doc-biblioref">2001</a>)</span>, o método <code>gap_stat</code> (<em>gap statistic</em>) compara variação total intra-grupo para diferentes valores de k com seus valores esperados sob alguma distribuição de referência. A estimativa dos clusters ótimos será o valor que maximiza a estatística de gap (isto é, que gera a maior estatística de gap).
<span class="math display" id="eq:gap">\[\begin{equation}
\mathrm{Gap}_{n}(k) = E^{*} \{ \mathrm{log}(W_k) \} - \mathrm{log}(W_k)
\tag{8.11}
\end{equation}\]</span>
- <span class="math inline">\(E^{*}\)</span> é o valor esperado sob uma amostra de tamanho <span class="math inline">\(n\)</span> da distribuição de referência<br />
- <span class="math inline">\(W_k = \sum_{j=1}^{k} \dfrac{1}{2n_j} D_j\)</span><br />
- <span class="math inline">\(D_j = \sum_{i,i&#39; \in C_j} d_{ii&#39;}\)</span><br />
- <span class="math inline">\(n_j = |C_j|\)</span></p>
<div class="sourceCode" id="cb708"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb708-1"><a href="aprendizadodemaquina.html#cb708-1"></a><span class="kw">fviz_nbclust</span>(iris2, kmeans, <span class="dt">method =</span> <span class="st">&#39;gap_stat&#39;</span>)</span></code></pre></div>
<pre><code>## Clustering k = 1,2,..., K.max (= 10): .. done
## Bootstrapping, b = 1,2,..., B (= 100)  [one &quot;.&quot; per sample]:
## .................................................. 50 
## .................................................. 100</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-172-1.png" width="672" /></p>

<div class="exercise">
<span id="exr:unnamed-chunk-173" class="exercise"><strong>Exercise 8.7  </strong></span>Leia:<br />
(a) a documentação da função <code>fviz_nbclust</code>.<br />
(b) <a href="https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods" class="uri">https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods</a>.<br />
(c) A Seção <em>e. Graphical Output Concerning Each Clustering</em>, pg. 83-88 de <span class="citation">(Rousseeuw and Kaufman <a href="#ref-rousseeuw1990finding" role="doc-biblioref">1990</a>)</span>.
</div>

<div class="exercise">
<span id="exr:unnamed-chunk-174" class="exercise"><strong>Exercise 8.8  </strong></span>Considere novamente o conjunto de dados <code>pib</code>.<br />
(a) Verifique as sugestões do número ótimo de grupos com os diferente métodos disponíveis na função <code>fviz_nbclust</code>.<br />
(b) Crie o grupamento que considerar mais adequado aos dados e apresente com a função <code>fviz_cluster</code>.<br />
(c) Compare os resultados com o Exercício 7.3.
</div>

<div class="exercise">
<span id="exr:unnamed-chunk-175" class="exercise"><strong>Exercise 8.9  </strong></span>Considere o conjunto de dados <code>drinks</code>, discutido no Capítulo 5.<br />
(a) Calcule as distâncias de Manhattan, euclidiana e de Minkowski com <span class="math inline">\(p=3\)</span>.<br />
(b) Obtenha os modelos hierárquicos utilizando as três distâncias do item (a). Você nota alguma diferença?<br />
(c) Obtenha a seleção inicial dos centróides a partir de proposta de <span class="citation">(Hartigan <a href="#ref-hartigan1975clustering" role="doc-biblioref">1975</a>)</span> apresentada na Equação <span class="math inline">\(\eqref{eq:j}\)</span>. Sugestão: escreva uma função que dependa dos dados e de <span class="math inline">\(k\)</span>, realizando alguma correção que considerar relevante.<br />
(d) Calcule os centróides dos grupos obtidos no item (c).<br />
(e) Calcule a <span class="math inline">\(VQI\)</span> dos grupos obtidos no item (c) a partir da Eq. <span class="math inline">\(\eqref{eq:vqi}\)</span>.<br />
(f) Calcule a <span class="math inline">\(SQT\)</span> dos grupos obtidos no item (c) a partir da Eq. <span class="math inline">\(\eqref{eq:sqt}\)</span>.<br />
(g) Verifique as sugestões do número ótimo de grupos com os diferente métodos disponíveis na função <code>fviz_nbclust</code>.<br />
(h) Crie o grupamento que considerar mais adequado aos dados e apresente com a função <code>fviz_cluster</code>.
</div>
<div class="sourceCode" id="cb710"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb710-1"><a href="aprendizadodemaquina.html#cb710-1"></a>dat &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&#39;http://www.filipezabala.com/data/drinks.txt&#39;</span>, <span class="dt">header =</span> T, <span class="dt">sep =</span> <span class="st">&#39;</span><span class="ch">\t</span><span class="st">&#39;</span>)</span></code></pre></div>

<div class="exercise">
<span id="exr:unnamed-chunk-177" class="exercise"><strong>Exercise 8.10  </strong></span>Considere o banco de dados sobre eficiência energética didcutido no Exemplo 5.2.<br />
(a) Calcule as distâncias de Manhattan, euclidiana e de Minkowski com <span class="math inline">\(p=3\)</span>.<br />
(b) Obtenha os modelos hierárquicos utilizando as três distâncias do item (a). (PODE DEMORAR!)<br />
(c) Verifique as sugestões do número ótimo de grupos com os diferente métodos disponíveis na função <code>fviz_nbclust</code>.<br />
(d) Crie o grupamento que considerar mais adequado aos dados e apresente com a função <code>fviz_cluster</code>.
</div>
<div class="sourceCode" id="cb711"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb711-1"><a href="aprendizadodemaquina.html#cb711-1"></a><span class="kw">library</span>(readxl)</span>
<span id="cb711-2"><a href="aprendizadodemaquina.html#cb711-2"></a>url1 &lt;-<span class="st"> &#39;http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx&#39;</span></span>
<span id="cb711-3"><a href="aprendizadodemaquina.html#cb711-3"></a><span class="kw">download.file</span>(url1, <span class="st">&#39;temp.xlsx&#39;</span>, <span class="dt">mode =</span> <span class="st">&#39;wb&#39;</span>)</span>
<span id="cb711-4"><a href="aprendizadodemaquina.html#cb711-4"></a>dat &lt;-<span class="st"> </span><span class="kw">read_excel</span>(<span class="st">&#39;temp.xlsx&#39;</span>)</span></code></pre></div>

</div>
</div>
</div>
</div>
<h3> Referências</h3>
<div id="refs" class="references">
<div id="ref-bishop1999bayesian">
<p>Bishop, Christopher M. 1999. “Bayesian PCA.” In <em>Advances in Neural Information Processing Systems</em>, 382–88. <a href="https://papers.nips.cc/paper/1549-bayesian-pca.pdf">https://papers.nips.cc/paper/1549-bayesian-pca.pdf</a>.</p>
</div>
<div id="ref-dua2019uci">
<p>Dua, Dheeru, and Casey Graff. 2019. “UCI Machine Learning Repository.” University of California, Irvine, School of Information; Computer Sciences. <a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>.</p>
</div>
<div id="ref-forgy1965cluster">
<p>Forgy, Edward W. 1965. “Cluster Analysis of Multivariate Data: Efficiency Versus Interpretability of Classifications.” <em>Biometrics</em> 21: 768–69.</p>
</div>
<div id="ref-hartigan1975clustering">
<p>Hartigan, John A. 1975. <em>Clustering Algorithms</em>. Wiley. <a href="https://people.inf.elte.hu/fekete/algoritmusok_msc/klaszterezes/John%20A.%20Hartigan-Clustering%20Algorithms-John%20Wiley%20&amp;%20Sons%20(1975).pdf">https://people.inf.elte.hu/fekete/algoritmusok_msc/klaszterezes/John%20A.%20Hartigan-Clustering%20Algorithms-John%20Wiley%20&amp;%20Sons%20(1975).pdf</a>.</p>
</div>
<div id="ref-hartigan1979algorithm">
<p>Hartigan, John A, and Manchek A Wong. 1979. “Algorithm AS 136: A K-Means Clustering Algorithm.” <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 28 (1): 100–108. <a href="https://www.labri.fr/perso/bpinaud/userfiles/downloads/hartigan_1979_kmeans.pdf">https://www.labri.fr/perso/bpinaud/userfiles/downloads/hartigan_1979_kmeans.pdf</a>.</p>
</div>
<div id="ref-hotelling1933analysis">
<p>Hotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” <em>Journal of Educational Psychology</em> 24 (6): 417. <a href="https://psycnet.apa.org/fulltext/1934-00645-001.pdf">https://psycnet.apa.org/fulltext/1934-00645-001.pdf</a>.</p>
</div>
<div id="ref-izbicki2020aprendizado">
<p>Izbicki, Rafael, and Tiago Mendonça dos Santos. 2020. <em>Aprendizado de Máquina: Uma Abordagem Estatística</em>. <a href="http://www.rizbicki.ufscar.br/ame/">http://www.rizbicki.ufscar.br/ame/</a>.</p>
</div>
<div id="ref-lloyd1982least">
<p>Lloyd, Stuart P. 1957. “Least Squares Quantization in PCM.” <em>Tecnical Note at Bell Laboratories in 1957, Published After in IEEE Transactions on Information Theory in 1982</em> 28 (2): 129–37. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1056489">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1056489</a>.</p>
</div>
<div id="ref-macqueen1967some">
<p>MacQueen, James, and others. 1967. “Some Methods for Classification and Analysis of Multivariate Observations.” In <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em>, 1:281–97. 14. Oakland, CA, USA. <a href="https://sci2s.ugr.es/keel/pdf/algorithm/congreso/1967-MacQueen-MSP.pdf">https://sci2s.ugr.es/keel/pdf/algorithm/congreso/1967-MacQueen-MSP.pdf</a>.</p>
</div>
<div id="ref-pearson1901on">
<p>Pearson, Karl. 1901. “On Lines and Planes of Closest Fit to Systems of Points in Space.” <em>Philosophical Magazine</em> 2 (11): 559–72. <a href="http://pca.narod.ru/pearson1901.pdf">http://pca.narod.ru/pearson1901.pdf</a>.</p>
</div>
<div id="ref-rousseeuw1990finding">
<p>Rousseeuw, Peter J, and L Kaufman. 1990. “Finding Groups in Data.” <em>Hoboken: Wiley Online Library</em>. <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470316801">https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470316801</a>.</p>
</div>
<div id="ref-sokal1962comparison">
<p>Sokal, Robert R, and F James Rohlf. 1962. “The Comparison of Dendrograms by Objective Methods.” <em>Taxon</em>, 33–40. <a href="https://www.jstor.org/stable/pdf/1217208.pdf">https://www.jstor.org/stable/pdf/1217208.pdf</a>.</p>
</div>
<div id="ref-thorndike1953belongs">
<p>Thorndike, Robert L. 1953. “Who Belongs in the Family?” <em>Psychometrika</em> 18 (4): 267–76. <a href="https://link.springer.com/content/pdf/10.1007%2FBF02289263.pdf">https://link.springer.com/content/pdf/10.1007%2FBF02289263.pdf</a>.</p>
</div>
<div id="ref-tibshirani2001estimating">
<p>Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data Set via the Gap Statistic.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 63 (2): 411–23. <a href="http://web.stanford.edu/~hastie/Papers/gap.pdf">http://web.stanford.edu/~hastie/Papers/gap.pdf</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="29">
<li id="fn29"><p>Exemplos baseados em <a href="https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html" class="uri">https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html</a>.<a href="aprendizadodemaquina.html#fnref29" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modelos-lineares.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="seriestemporais.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
